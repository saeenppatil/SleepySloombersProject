{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFiEb2pGPNDo",
        "outputId": "9f5c17b7-820c-4bba-c7b2-635fff710e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.6.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet torch==2.0.1 torchtext==0.15.2\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import re\n",
        "\n",
        "# === Step 1: Tokenization Setup ===\n",
        "tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")  # Uses SpaCy tokenizer\n",
        "\n",
        "SPECIAL_TOKENS = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "SOS_IDX = 2\n",
        "EOS_IDX = 3\n",
        "\n",
        "# === Step 2: Load and Tokenize Jokes ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!unzip \"/content/drive/MyDrive/ECS 189G/stage_4_data.zip\" -d /content/\n",
        "\n",
        "DATA_PATH = \"/content/stage_4_data/text_generation/data\"\n",
        "\n",
        "def clean_line(line):\n",
        "    line = line.lower().strip()\n",
        "    line = re.sub(r'https?://\\S+|www\\.\\S+', '', line)\n",
        "    line = re.sub(r'\\s+', ' ', line)\n",
        "    return line\n",
        "\n",
        "with open(DATA_PATH, 'r') as f:\n",
        "    raw_lines = f.readlines()\n",
        "\n",
        "tokenized_jokes = []\n",
        "for line in raw_lines:\n",
        "    line = clean_line(line)\n",
        "    tokens = tokenizer(line)\n",
        "    if len(tokens) >= 4 and len(tokens) <= 40:\n",
        "        tokenized_jokes.append(['<sos>'] + tokens + ['<eos>'])\n",
        "\n",
        "# === Step 3: Build Vocab (GloVe + Special Tokens) ===\n",
        "glove_base = GloVe(name=\"6B\", dim=300)\n",
        "special_vectors = torch.randn(len(SPECIAL_TOKENS), glove_base.dim) * 0.1\n",
        "vocab_stoi = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
        "offset = len(SPECIAL_TOKENS)\n",
        "\n",
        "for word in glove_base.stoi:\n",
        "    vocab_stoi[word] = glove_base.stoi[word] + offset\n",
        "\n",
        "# Build itos mapping and full embedding matrix\n",
        "vocab_itos = [None] * len(vocab_stoi)\n",
        "for word, idx in vocab_stoi.items():\n",
        "    vocab_itos[idx] = word\n",
        "\n",
        "glove_vectors = torch.cat([special_vectors, glove_base.vectors], dim=0)\n",
        "\n",
        "class GloveVocab:\n",
        "    def __init__(self, stoi, itos, vectors):\n",
        "        self.stoi = stoi\n",
        "        self.itos = itos\n",
        "        self.vectors = vectors\n",
        "        self.dim = vectors.shape[1]\n",
        "\n",
        "vocab = GloveVocab(vocab_stoi, vocab_itos, glove_vectors)\n",
        "\n",
        "# === Step 4: Encode Dataset ===\n",
        "encoded_jokes = [[vocab.stoi.get(tok, UNK_IDX) for tok in joke] for joke in tokenized_jokes]\n",
        "\n",
        "class JokeDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        seq = torch.tensor(self.data[i], dtype=torch.long)\n",
        "        return seq[:-1], seq[1:]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
        "    return inputs, targets\n",
        "\n",
        "loader = DataLoader(JokeDataset(encoded_jokes), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# === Step 5: Model ===\n",
        "class JokeLSTM(nn.Module):\n",
        "    def __init__(self, embedding_weights, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        vocab_size, emb_dim = embedding_weights.shape\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights.detach(), freeze=False, padding_idx=PAD_IDX)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        out, _ = self.lstm(emb)\n",
        "        return self.fc(out)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = JokeLSTM(vocab.vectors).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# === Step 6: Training Loop ===\n",
        "def train_model(model, loader, epochs=200):\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        total_loss = 0\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}/{epochs} | Loss: {total_loss / len(loader):.4f}\")\n",
        "\n",
        "train_model(model, loader)\n",
        "\n",
        "# === Step 7: Joke Generation ===\n",
        "def generate_joke(model, prompt, max_len=40, temperature=1.0, top_k=30):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt.lower())\n",
        "    idxs = [vocab.stoi.get(tok, UNK_IDX) for tok in tokens]\n",
        "    idxs = [SOS_IDX] + idxs\n",
        "    input_tensor = torch.tensor(idxs, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    generated = tokens.copy()\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_tensor)[0, -1] / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            if top_k:\n",
        "                top_probs, top_idx = torch.topk(probs, top_k)\n",
        "                next_token = top_idx[torch.multinomial(top_probs, 1)].item()\n",
        "            else:\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        next_word = vocab.itos[next_token]\n",
        "        if next_word == '<eos>':\n",
        "            break\n",
        "        generated.append(next_word)\n",
        "        input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]]).to(device)], dim=1)\n",
        "\n",
        "    return ' '.join(generated)\n",
        "\n",
        "# === Step 8: Try It Out ===\n",
        "print(\"\\nGENERATED JOKES:\")\n",
        "print(generate_joke(model, \"what do you call\"))\n",
        "print(generate_joke(model, \"why did the chicken\"))\n",
        "print(generate_joke(model, \"how do you stop a robot\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfo0zGIbPe8h",
        "outputId": "bcea3c46-a011-4613-c12d-41c65d3bf836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 399999/400000 [00:42<00:00, 9340.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200 | Loss: 8.4462\n",
            "Epoch 2/200 | Loss: 6.0401\n",
            "Epoch 3/200 | Loss: 5.7326\n",
            "Epoch 4/200 | Loss: 5.6131\n",
            "Epoch 5/200 | Loss: 5.5376\n",
            "Epoch 6/200 | Loss: 5.4741\n",
            "Epoch 7/200 | Loss: 5.4158\n",
            "Epoch 8/200 | Loss: 5.3381\n",
            "Epoch 9/200 | Loss: 5.2585\n",
            "Epoch 10/200 | Loss: 5.1681\n",
            "Epoch 11/200 | Loss: 5.0893\n",
            "Epoch 12/200 | Loss: 5.0134\n",
            "Epoch 13/200 | Loss: 4.9541\n",
            "Epoch 14/200 | Loss: 4.8945\n",
            "Epoch 15/200 | Loss: 4.8404\n",
            "Epoch 16/200 | Loss: 4.7724\n",
            "Epoch 17/200 | Loss: 4.7094\n",
            "Epoch 18/200 | Loss: 4.6478\n",
            "Epoch 19/200 | Loss: 4.5819\n",
            "Epoch 20/200 | Loss: 4.5067\n",
            "Epoch 21/200 | Loss: 4.4521\n",
            "Epoch 22/200 | Loss: 4.3907\n",
            "Epoch 23/200 | Loss: 4.3295\n",
            "Epoch 24/200 | Loss: 4.2769\n",
            "Epoch 25/200 | Loss: 4.2195\n",
            "Epoch 26/200 | Loss: 4.1614\n",
            "Epoch 27/200 | Loss: 4.1099\n",
            "Epoch 28/200 | Loss: 4.0632\n",
            "Epoch 29/200 | Loss: 4.0199\n",
            "Epoch 30/200 | Loss: 3.9703\n",
            "Epoch 31/200 | Loss: 3.9262\n",
            "Epoch 32/200 | Loss: 3.8689\n",
            "Epoch 33/200 | Loss: 3.8349\n",
            "Epoch 34/200 | Loss: 3.7902\n",
            "Epoch 35/200 | Loss: 3.7452\n",
            "Epoch 36/200 | Loss: 3.7050\n",
            "Epoch 37/200 | Loss: 3.6609\n",
            "Epoch 38/200 | Loss: 3.6172\n",
            "Epoch 39/200 | Loss: 3.5803\n",
            "Epoch 40/200 | Loss: 3.5319\n",
            "Epoch 41/200 | Loss: 3.4881\n",
            "Epoch 42/200 | Loss: 3.4452\n",
            "Epoch 43/200 | Loss: 3.4068\n",
            "Epoch 44/200 | Loss: 3.3632\n",
            "Epoch 45/200 | Loss: 3.3174\n",
            "Epoch 46/200 | Loss: 3.2721\n",
            "Epoch 47/200 | Loss: 3.2255\n",
            "Epoch 48/200 | Loss: 3.1934\n",
            "Epoch 49/200 | Loss: 3.1441\n",
            "Epoch 50/200 | Loss: 3.1048\n",
            "Epoch 51/200 | Loss: 3.0634\n",
            "Epoch 52/200 | Loss: 3.0211\n",
            "Epoch 53/200 | Loss: 2.9796\n",
            "Epoch 54/200 | Loss: 2.9361\n",
            "Epoch 55/200 | Loss: 2.8981\n",
            "Epoch 56/200 | Loss: 2.8606\n",
            "Epoch 57/200 | Loss: 2.8169\n",
            "Epoch 58/200 | Loss: 2.7827\n",
            "Epoch 59/200 | Loss: 2.7366\n",
            "Epoch 60/200 | Loss: 2.6920\n",
            "Epoch 61/200 | Loss: 2.6490\n",
            "Epoch 62/200 | Loss: 2.6141\n",
            "Epoch 63/200 | Loss: 2.5715\n",
            "Epoch 64/200 | Loss: 2.5358\n",
            "Epoch 65/200 | Loss: 2.4935\n",
            "Epoch 66/200 | Loss: 2.4463\n",
            "Epoch 67/200 | Loss: 2.4133\n",
            "Epoch 68/200 | Loss: 2.3708\n",
            "Epoch 69/200 | Loss: 2.3365\n",
            "Epoch 70/200 | Loss: 2.2977\n",
            "Epoch 71/200 | Loss: 2.2568\n",
            "Epoch 72/200 | Loss: 2.2142\n",
            "Epoch 73/200 | Loss: 2.1814\n",
            "Epoch 74/200 | Loss: 2.1454\n",
            "Epoch 75/200 | Loss: 2.1136\n",
            "Epoch 76/200 | Loss: 2.0750\n",
            "Epoch 77/200 | Loss: 2.0434\n",
            "Epoch 78/200 | Loss: 2.0121\n",
            "Epoch 79/200 | Loss: 1.9719\n",
            "Epoch 80/200 | Loss: 1.9346\n",
            "Epoch 81/200 | Loss: 1.9070\n",
            "Epoch 82/200 | Loss: 1.8704\n",
            "Epoch 83/200 | Loss: 1.8422\n",
            "Epoch 84/200 | Loss: 1.8073\n",
            "Epoch 85/200 | Loss: 1.7789\n",
            "Epoch 86/200 | Loss: 1.7472\n",
            "Epoch 87/200 | Loss: 1.7130\n",
            "Epoch 88/200 | Loss: 1.6901\n",
            "Epoch 89/200 | Loss: 1.6605\n",
            "Epoch 90/200 | Loss: 1.6234\n",
            "Epoch 91/200 | Loss: 1.5945\n",
            "Epoch 92/200 | Loss: 1.5648\n",
            "Epoch 93/200 | Loss: 1.5445\n",
            "Epoch 94/200 | Loss: 1.5193\n",
            "Epoch 95/200 | Loss: 1.4911\n",
            "Epoch 96/200 | Loss: 1.4657\n",
            "Epoch 97/200 | Loss: 1.4417\n",
            "Epoch 98/200 | Loss: 1.4137\n",
            "Epoch 99/200 | Loss: 1.3931\n",
            "Epoch 100/200 | Loss: 1.3701\n",
            "Epoch 101/200 | Loss: 1.3406\n",
            "Epoch 102/200 | Loss: 1.3204\n",
            "Epoch 103/200 | Loss: 1.3002\n",
            "Epoch 104/200 | Loss: 1.2780\n",
            "Epoch 105/200 | Loss: 1.2509\n",
            "Epoch 106/200 | Loss: 1.2380\n",
            "Epoch 107/200 | Loss: 1.2212\n",
            "Epoch 108/200 | Loss: 1.1997\n",
            "Epoch 109/200 | Loss: 1.1816\n",
            "Epoch 110/200 | Loss: 1.1589\n",
            "Epoch 111/200 | Loss: 1.1406\n",
            "Epoch 112/200 | Loss: 1.1238\n",
            "Epoch 113/200 | Loss: 1.1034\n",
            "Epoch 114/200 | Loss: 1.0887\n",
            "Epoch 115/200 | Loss: 1.0723\n",
            "Epoch 116/200 | Loss: 1.0562\n",
            "Epoch 117/200 | Loss: 1.0362\n",
            "Epoch 118/200 | Loss: 1.0208\n",
            "Epoch 119/200 | Loss: 1.0128\n",
            "Epoch 120/200 | Loss: 0.9904\n",
            "Epoch 121/200 | Loss: 0.9778\n",
            "Epoch 122/200 | Loss: 0.9632\n",
            "Epoch 123/200 | Loss: 0.9493\n",
            "Epoch 124/200 | Loss: 0.9366\n",
            "Epoch 125/200 | Loss: 0.9215\n",
            "Epoch 126/200 | Loss: 0.9095\n",
            "Epoch 127/200 | Loss: 0.9006\n",
            "Epoch 128/200 | Loss: 0.8811\n",
            "Epoch 129/200 | Loss: 0.8699\n",
            "Epoch 130/200 | Loss: 0.8565\n",
            "Epoch 131/200 | Loss: 0.8426\n",
            "Epoch 132/200 | Loss: 0.8341\n",
            "Epoch 133/200 | Loss: 0.8241\n",
            "Epoch 134/200 | Loss: 0.8137\n",
            "Epoch 135/200 | Loss: 0.7985\n",
            "Epoch 136/200 | Loss: 0.7913\n",
            "Epoch 137/200 | Loss: 0.7806\n",
            "Epoch 138/200 | Loss: 0.7723\n",
            "Epoch 139/200 | Loss: 0.7639\n",
            "Epoch 140/200 | Loss: 0.7516\n",
            "Epoch 141/200 | Loss: 0.7448\n",
            "Epoch 142/200 | Loss: 0.7346\n",
            "Epoch 143/200 | Loss: 0.7343\n",
            "Epoch 144/200 | Loss: 0.7216\n",
            "Epoch 145/200 | Loss: 0.7082\n",
            "Epoch 146/200 | Loss: 0.6991\n",
            "Epoch 147/200 | Loss: 0.6934\n",
            "Epoch 148/200 | Loss: 0.6909\n",
            "Epoch 149/200 | Loss: 0.6822\n",
            "Epoch 150/200 | Loss: 0.6703\n",
            "Epoch 151/200 | Loss: 0.6635\n",
            "Epoch 152/200 | Loss: 0.6613\n",
            "Epoch 153/200 | Loss: 0.6552\n",
            "Epoch 154/200 | Loss: 0.6482\n",
            "Epoch 155/200 | Loss: 0.6384\n",
            "Epoch 156/200 | Loss: 0.6327\n",
            "Epoch 157/200 | Loss: 0.6256\n",
            "Epoch 158/200 | Loss: 0.6215\n",
            "Epoch 159/200 | Loss: 0.6171\n",
            "Epoch 160/200 | Loss: 0.6121\n",
            "Epoch 161/200 | Loss: 0.6061\n",
            "Epoch 162/200 | Loss: 0.6029\n",
            "Epoch 163/200 | Loss: 0.5967\n",
            "Epoch 164/200 | Loss: 0.5889\n",
            "Epoch 165/200 | Loss: 0.5821\n",
            "Epoch 166/200 | Loss: 0.5807\n",
            "Epoch 167/200 | Loss: 0.5805\n",
            "Epoch 168/200 | Loss: 0.5736\n",
            "Epoch 169/200 | Loss: 0.5701\n",
            "Epoch 170/200 | Loss: 0.5676\n",
            "Epoch 171/200 | Loss: 0.5617\n",
            "Epoch 172/200 | Loss: 0.5565\n",
            "Epoch 173/200 | Loss: 0.5539\n",
            "Epoch 174/200 | Loss: 0.5478\n",
            "Epoch 175/200 | Loss: 0.5472\n",
            "Epoch 176/200 | Loss: 0.5411\n",
            "Epoch 177/200 | Loss: 0.5390\n",
            "Epoch 178/200 | Loss: 0.5335\n",
            "Epoch 179/200 | Loss: 0.5303\n",
            "Epoch 180/200 | Loss: 0.5282\n",
            "Epoch 181/200 | Loss: 0.5247\n",
            "Epoch 182/200 | Loss: 0.5192\n",
            "Epoch 183/200 | Loss: 0.5201\n",
            "Epoch 184/200 | Loss: 0.5160\n",
            "Epoch 185/200 | Loss: 0.5141\n",
            "Epoch 186/200 | Loss: 0.5100\n",
            "Epoch 187/200 | Loss: 0.5088\n",
            "Epoch 188/200 | Loss: 0.5087\n",
            "Epoch 189/200 | Loss: 0.5086\n",
            "Epoch 190/200 | Loss: 0.5038\n",
            "Epoch 191/200 | Loss: 0.4985\n",
            "Epoch 192/200 | Loss: 0.4981\n",
            "Epoch 193/200 | Loss: 0.4956\n",
            "Epoch 194/200 | Loss: 0.4978\n",
            "Epoch 195/200 | Loss: 0.4939\n",
            "Epoch 196/200 | Loss: 0.4886\n",
            "Epoch 197/200 | Loss: 0.4850\n",
            "Epoch 198/200 | Loss: 0.4862\n",
            "Epoch 199/200 | Loss: 0.4842\n",
            "Epoch 200/200 | Loss: 0.4848\n",
            "\n",
            "GENERATED JOKES:\n",
            "what do you call an arcade in eastern europe ? czech - e - cheese \"\n",
            "why did the chicken lay an egg ? ( quoted from daughter at age 3 ) to get food for her babies ! \"\n",
            "how do you stop a robot cross the road ? ... it was two - tired . \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Extended Prompt List for Joke Generation ===\n",
        "joke_prompts = [\n",
        "    # Classic joke starters\n",
        "    \"what do you call a\",\n",
        "    \"why did the chicken\",\n",
        "    \"why did the\",\n",
        "    \"what did the\",\n",
        "    \"how do you\",\n",
        "    \"how do you make a\",\n",
        "    \"how do you stop a\",\n",
        "    \"what's the difference between\",\n",
        "    \"what's worse than\",\n",
        "\n",
        "    # Question-based setups\n",
        "    \"why don't\",\n",
        "    \"why can't\",\n",
        "    \"why do\",\n",
        "    \"what happens when\",\n",
        "    \"what do you get when\",\n",
        "    \"what do you get if\",\n",
        "\n",
        "    # Character-based setups\n",
        "    \"a man walks into\",\n",
        "    \"two guys walk into\",\n",
        "    \"the bartender says\",\n",
        "    \"the doctor said\",\n",
        "    \"my wife said\",\n",
        "\n",
        "    # Animal jokes\n",
        "    \"what do you call a cow\",\n",
        "    \"what do you call a dog\",\n",
        "    \"what do you call a fish\",\n",
        "    \"why don't elephants\",\n",
        "    \"what did the cat\",\n",
        "\n",
        "    # Profession jokes\n",
        "    \"why don't scientists\",\n",
        "    \"what did the lawyer\",\n",
        "    \"the teacher asked\",\n",
        "    \"why do programmers\",\n",
        "\n",
        "    # Food jokes\n",
        "    \"what did the grape\",\n",
        "    \"why did the banana\",\n",
        "    \"what's a skeleton's favorite\",\n",
        "\n",
        "    # Tech/modern jokes\n",
        "    \"why do robots\",\n",
        "    \"what did the computer\",\n",
        "    \"why don't phones\",\n",
        "\n",
        "    # Incomplete phrases to test completion\n",
        "    \"knock knock\",\n",
        "    \"i told my wife\",\n",
        "    \"my doctor said\",\n",
        "    \"yesterday i went to\"\n",
        "]\n",
        "\n",
        "# Print to confirm it's loaded\n",
        "print(f\"Loaded {len(joke_prompts)} prompts for testing.\")\n",
        "for i in range(5):\n",
        "    print(f\"- {joke_prompts[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8PLetm1Uc6V",
        "outputId": "86e15f99-bbc5-4847-f1e1-eb2f422805cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 39 prompts for testing.\n",
            "- what do you call a\n",
            "- why did the chicken\n",
            "- why did the\n",
            "- what did the\n",
            "- how do you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Number of different jokes to generate per prompt\n",
        "n_generations = 2\n",
        "\n",
        "# Sample from the loaded prompts\n",
        "for prompt in joke_prompts:\n",
        "    print(f\"\\nPrompt: '{prompt}'\")\n",
        "    for i in range(n_generations):\n",
        "        joke = generate_joke(model, prompt, max_len=35, temperature=0.7, top_k=40)\n",
        "        print(f\"  {i+1}: {joke}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl7ZglC_UeN5",
        "outputId": "1bd26c02-b4ff-41d3-ca78-3096fabb0104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'what do you call a'\n",
            "  1: what do you call a midget psychic who just escaped from prison ? a small medium at large \"\n",
            "  2: what do you call a cow with one leg ? steak . \"\n",
            "\n",
            "Prompt: 'why did the chicken'\n",
            "  1: why did the chicken cross the road ? to get away from gordon ramsey \"\n",
            "  2: why did the chicken cross the road ? to get to the moron 's house . * knock knock * <unk> <unk> ? * the chicken ... * \"\n",
            "\n",
            "Prompt: 'why did the'\n",
            "  1: why did the fish go when it needed a filling ? well , it was two - tired \"\n",
            "  2: why did the boy take a pencil and paper to bed ? he was told to draw the curtains before going to sleep . \"\n",
            "\n",
            "Prompt: 'what did the'\n",
            "  1: what did the bicycle fall over ? because it was two - tired \"\n",
            "  2: what did the chicken lay dyslexia ? \"\n",
            "\n",
            "Prompt: 'how do you'\n",
            "  1: how do you catch a bra ? you set a booby trap . \"\n",
            "  2: how do you call a group of people standing in the arctic circle ? a finnish line . \"\n",
            "\n",
            "Prompt: 'how do you make a'\n",
            "  1: how do you make a squid laugh ? ten tickles . \"\n",
            "  2: how do you make a kleenex dance ? you put a little boogie in it ! \"\n",
            "\n",
            "Prompt: 'how do you stop a'\n",
            "  1: how do you stop a corny bird ? he 's reluctant to make a bear . \"\n",
            "  2: how do you stop a door with the flu ? one 's the world 's greatest composers . \"\n",
            "\n",
            "Prompt: 'what's the difference between'\n",
            "  1: what 's the difference between a firstborn prince and a baseball ? a baseball is thrown to the air . \"\n",
            "  2: what 's the difference between a bag of chips and a duck with the flu ? one 's a quick snack and the other 's a sick quack ! \"\n",
            "\n",
            "Prompt: 'what's worse than'\n",
            "  1: what 's worse than a centipede ? a potato new jersey ! \"\n",
            "  2: what 's worse than his construction homework ? it 's making a time . \"\n",
            "\n",
            "Prompt: 'why don't'\n",
            "  1: why do n't serve time travelers here \" \" a time traveler walks into a bar . \"\n",
            "  2: why do n't cannibals like clowns ? they taste funny ! \"\n",
            "\n",
            "Prompt: 'why can't'\n",
            "  1: why ca n't possibly very company when he said he wanted to buy all the same ... \"\n",
            "  2: why ca n't possibly describe how beautiful you are ... but numbers can 4/10 \"\n",
            "\n",
            "Prompt: 'why do'\n",
            "  1: why do you call a blind - artist who minored in psychology ? sigmund fraud \"\n",
            "  2: why do you call a cow with 2 legs ? lean beef . \"\n",
            "\n",
            "Prompt: 'what happens when'\n",
            "  1: what happens when a spoon and fork get into a fight ? <unk> \"\n",
            "  2: what happens when a spoon and fork get into a fight ? <unk> \"\n",
            "\n",
            "Prompt: 'what do you get when'\n",
            "  1: what do you get when you sit on a potato ? a potato wedge ! ( i made the refrigerator after it 's abroad . \"\n",
            "  2: what do you get when you cross a crocodile with a cartridge ? a snapshot . \"\n",
            "\n",
            "Prompt: 'what do you get if'\n",
            "  1: what do you get if you cross a firecracker and a duck ? a <unk> . \"\n",
            "  2: what do you get if you cross a crocodile with a cartridge ? a snapshot . \"\n",
            "\n",
            "Prompt: 'a man walks into'\n",
            "  1: a man walks into a bar ... all of a martini . the bartender asks \" \" olive ' er twist ? \" \" \"\n",
            "  2: a man walks into a bar ... the neck of him says , and there are just a <unk> . \"\n",
            "\n",
            "Prompt: 'two guys walk into'\n",
            "  1: two guys walk into a bar ... the third one ducks . \"\n",
            "  2: two guys walk into a bar ... the third one ducks . \"\n",
            "\n",
            "Prompt: 'the bartender says'\n",
            "  1: the bartender says to a pirate your best friend ? you buy it me , you 'll get terrible , what you call a pool . its ) \"\n",
            "  2: the bartender says to a watch . it 's too very eight . \" \" marshmallow . \" \" \"\n",
            "\n",
            "Prompt: 'the doctor said'\n",
            "  1: the doctor said to the block between each want to make a video of a white liquid ? brushing your point \"\n",
            "  2: the doctor said to the same thing after now . \"\n",
            "\n",
            "Prompt: 'my wife said'\n",
            "  1: my wife said i could n't you want your life ... but you 've calmed down . \" \" i 'm so hungry i could n't concentrate . \" \" i 'm so hungry i could eat a\n",
            "  2: my wife said why 's the cost be after he was in his car sorry but now he got outstanding in his head . \"\n",
            "\n",
            "Prompt: 'what do you call a cow'\n",
            "  1: what do you call a cow with only two legs ? lean beef ! \"\n",
            "  2: what do you call a cow with no legs ? ground beef . \"\n",
            "\n",
            "Prompt: 'what do you call a dog'\n",
            "  1: what do you call a dog with no legs ? do you do them ? a <unk> ! \"\n",
            "  2: what do you call a dog with no legs ? do n't bother , he 's gon na lure him in ? \"\n",
            "\n",
            "Prompt: 'what do you call a fish'\n",
            "  1: what do you call a fish with no eyes ? a fsh \"\n",
            "  2: what do you call a fish with no eyes ? a fsh \"\n",
            "\n",
            "Prompt: 'why don't elephants'\n",
            "  1: why do n't elephants ride behind ? to the retail pie . \"\n",
            "  2: why do n't elephants finish key when because the boot flew past birth through to the crypt tonight . \"\n",
            "\n",
            "Prompt: 'what did the cat'\n",
            "  1: what did the cat go the the number eight ? because he was always lost at the pickle . \"\n",
            "  2: what did the cat go to the dentist ? it needed a filling . \"\n",
            "\n",
            "Prompt: 'why don't scientists'\n",
            "  1: why do n't scientists give a bear for too ... ... and it on a high turnover rate . \"\n",
            "  2: why do n't scientists have a little meteor ? * tips ... you 're * body ! * * * * <unk> ' ! * * * jimmy carr * * \"\n",
            "\n",
            "Prompt: 'what did the lawyer'\n",
            "  1: what did the lawyer for u2 ? he was a light . \" \" well love so i 'm sure i have bored boars boring boards . ' \" \" elementary , my dear watson . \" \" \"\n",
            "  2: what did the lawyer be on the plane 's the clock ? because she 's fully recovered . \"\n",
            "\n",
            "Prompt: 'the teacher asked'\n",
            "  1: the teacher asked , to find their sheep ? he wanted to transcend dental medication . \"\n",
            "  2: the teacher asked to a truck off a tree ... it 's 1920 x 1080i . \"\n",
            "\n",
            "Prompt: 'why do programmers'\n",
            "  1: why do programmers like on the winter ? because they dress in lairs . \"\n",
            "  2: why do programmers like leapfrog ? teeth \" \" farts may my prints will tell you , bach ! \" \" <unk> i yam . \" \" \"\n",
            "\n",
            "Prompt: 'what did the grape'\n",
            "  1: what did the grape say when it got stepped on ? nothing , it just gave a little wine \"\n",
            "  2: what did the grape say when it got stepped on ? nothing , it just gave a little wine \"\n",
            "\n",
            "Prompt: 'why did the banana'\n",
            "  1: why did the banana get like to get into a pie ? somewhere to be a <unk> pact . \"\n",
            "  2: why did the banana get like to get into a pie ? somewhere by them love - <unk> joke ) \"\n",
            "\n",
            "Prompt: 'what's a skeleton's favorite'\n",
            "  1: what 's a skeleton 's favorite letter ? the alphabet is them . \"\n",
            "  2: what 's a skeleton 's favorite genre of music ? pop . \"\n",
            "\n",
            "Prompt: 'why do robots'\n",
            "  1: why do robots have such one together ? they get a boogie in it ! ( not sure of the spelling , heard it from a bowl and god say , and the streets have no name .\n",
            "  2: why do robots have such one together under the opportunity ? in the space - <unk> ! \"\n",
            "\n",
            "Prompt: 'what did the computer'\n",
            "  1: what did the computer get fun when you find two whales ? whale whale a little .... \"\n",
            "  2: what did the computer get serve the bear say to his son it ? he 's time much right \"\n",
            "\n",
            "Prompt: 'why don't phones'\n",
            "  1: why do n't phones a vampire from the window ? they 're jerks . \"\n",
            "  2: why do n't phones a right bones at the clock ? because it major n't you * . \"\n",
            "\n",
            "Prompt: 'knock knock'\n",
            "  1: knock knock ! * * who 's there ? * * * tank * * * * <unk> who , * * * * <unk> who , , , , ? * * * how do they\n",
            "  2: knock knock ... * * who 's there ? * * * tank * * * tank who ? * * * you 're welcome * \"\n",
            "\n",
            "Prompt: 'i told my wife'\n",
            "  1: i told my wife ... but i felt sick empty bottles , go that go out of the neck of so the first were awful when the reception was running ? i feel got . \"\n",
            "  2: i told my wife ... but i know wot to see ' b.c . ' on them now really take in his head . \"\n",
            "\n",
            "Prompt: 'my doctor said'\n",
            "  1: my doctor said to all the same thing . \"\n",
            "  2: my doctor said to just going a right joke . but i 'm sick though , i think got a little forced . \"\n",
            "\n",
            "Prompt: 'yesterday i went to'\n",
            "  1: yesterday i went to a clairvoyants meeting the other day but i guess you were having dinar . \"\n",
            "  2: yesterday i went to a seafood knock ! i 'm all . but it 's a flick of the wrist ! \"\n"
          ]
        }
      ]
    }
  ]
}